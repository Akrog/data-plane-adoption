[id="deploying-the-block-storage-services_{context}"]

= Deploying the Block Storage services

Assuming you have already stopped {block_storage_first_ref} services, prepared the {OpenShift} nodes,
deployed the {rhos_prev_long} ({OpenStackShort}) operators and a bare {OpenStackShort} manifest, and migrated the
database, and prepared the patch manifest with the {block_storage} configuration,
you must apply the patch and wait for the operator to apply the changes and deploy the Block Storage services.

.Prerequisites

* Previous Adoption steps completed. Notably, {block_storage} must have been
stopped and the service databases must already be imported into the control plane MariaDB.
* {identity_service_first_ref} and {key_manager_first_ref} should be already adopted.
* Storage network has been properly configured on the {OpenShiftShort} cluster.
* You need the contents of `cinder.conf` file. Download the file so that you can access it locally:
+
----
$CONTROLLER1_SSH cat /var/lib/config-data/puppet-generated/cinder/etc/cinder/cinder.conf > cinder.conf
----
//No new environmental variables need to be defined, though you use the
//`CONTROLLER1_SSH` that was defined in a previous step for the pre-checks.
//kgilliga: I commented this out for now because I'm not sure why "use the
//`CONTROLLER1_SSH` that was defined in a previous step for the pre-checks" is relevant.

.Procedure

. It is recommended to write the patch manifest into a file, for example
`cinder.patch` and then apply it with something like:
+
----
oc patch openstackcontrolplane openstack --type=merge --patch-file=cinder.patch
----
+
For example, for the RBD deployment from the Development Guide the
`cinder.patch` would look like this:
+
[source,yaml]
----
spec:
  extraMounts:
  - extraVol:
    - extraVolType: Ceph
      mounts:
      - mountPath: /etc/ceph
        name: ceph
        readOnly: true
      propagation:
      - CinderVolume
      - CinderBackup
      - Glance
      volumes:
      - name: ceph
        projected:
          sources:
          - secret:
              name: ceph-conf-files
  cinder:
    enabled: true
    apiOverride:
      route: {}
    template:
      databaseInstance: openstack
      databaseAccount: cinder
      secret: osp-secret
      cinderAPI:
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: 172.17.0.80
              spec:
                type: LoadBalancer
        replicas: 1
        customServiceConfig: |
          [DEFAULT]
          default_volume_type=tripleo
      cinderScheduler:
        replicas: 1
      cinderBackup:
        networkAttachments:
        - storage
        replicas: 1
        customServiceConfig: |
          [DEFAULT]
          backup_driver=cinder.backup.drivers.ceph.CephBackupDriver
          backup_ceph_conf=/etc/ceph/ceph.conf
          backup_ceph_user=openstack
          backup_ceph_pool=backups
      cinderVolumes:
        ceph:
          networkAttachments:
          - storage
          replicas: 1
          customServiceConfig: |
            [tripleo_ceph]
            backend_host=hostgroup
            volume_backend_name=tripleo_ceph
            volume_driver=cinder.volume.drivers.rbd.RBDDriver
            rbd_ceph_conf=/etc/ceph/ceph.conf
            rbd_user=openstack
            rbd_pool=volumes
            rbd_flatten_volume_from_snapshot=False
            report_discard_supported=True
----

. Once the services have been deployed you need to clean up the old scheduler
and backup services which will appear as being down while you have others that appear as being up:
+
----
openstack volume service list

+------------------+------------------------+------+---------+-------+----------------------------+
| Binary           | Host                   | Zone | Status  | State | Updated At                 |
+------------------+------------------------+------+---------+-------+----------------------------+
| cinder-backup    | standalone.localdomain | nova | enabled | down  | 2023-06-28T11:00:59.000000 |
| cinder-scheduler | standalone.localdomain | nova | enabled | down  | 2023-06-28T11:00:29.000000 |
| cinder-volume    | hostgroup@tripleo_ceph | nova | enabled | up    | 2023-06-28T17:00:03.000000 |
| cinder-scheduler | cinder-scheduler-0     | nova | enabled | up    | 2023-06-28T17:00:02.000000 |
| cinder-backup    | cinder-backup-0        | nova | enabled | up    | 2023-06-28T17:00:01.000000 |
+------------------+------------------------+------+---------+-------+----------------------------+
----

. In this case you need to remove services for hosts `standalone.localdomain`
+
----
oc exec -it cinder-scheduler-0 -- cinder-manage service remove cinder-backup standalone.localdomain
oc exec -it cinder-scheduler-0 -- cinder-manage service remove cinder-scheduler standalone.localdomain
----
+
The reason why we haven't preserved the name of the backup service is because
we have taken the opportunity to change its configuration to support
Active-Active, even though we are not doing so right now because we have 1
replica.
//kgilliga: The above paragraph is confusing. Who changed the configuration? Unclear on what the replica refers to.

. Now that the Block Storage services are running, the DB schema migration has been completed and you can proceed to apply the DB data migrations.
While it is not necessary to run these data migrations at this precise moment,
because you can run them right before the next upgrade, for adoption it is best to run them now to make sure there are no issues before running production workloads on the deployment.
+
The command to run the DB data migrations is:
+
----
oc exec -it cinder-scheduler-0 -- cinder-manage db online_data_migrations
----

.Verification

Before you can run any checks you need to set the right cloud configuration for
the `openstack` command to be able to connect to your {OpenShiftShort} control plane.

. Ensure that the `openstack` alias is defined:
+
----
alias openstack="oc exec -t openstackclient -- openstack"
----

Now you can run a set of tests to confirm that the deployment is using your
old database contents:

. See that {block_storage} endpoints are defined and pointing to the control plane FQDNs:
+
----
openstack endpoint list --service cinderv3
----

. Check that the Block Storage services are running and up. The API won't show but if
you get a response you know it's up as well:
+
----
openstack volume service list
----

. Check that your old volume types, volumes, snapshots, and backups are there:
+
----
openstack volume type list
openstack volume list
openstack volume snapshot list
openstack volume backup list
----

To confirm that the configuration is working, the following basic operations are recommended:

. Create a volume from an image to check that the connection to {image_service_first_ref} is working.
+
----
openstack volume create --image cirros --bootable --size 1 disk_new
----

. Backup the old attached volume to a new backup. Example:
+
----
openstack --os-volume-api-version 3.47 volume create --backup backup restored
----

[NOTE]
You do not boot a {compute_service_first_ref} instance using the new volume from image or try to detach the old volume because {compute_service} and the {block_storage} are still not connected.

