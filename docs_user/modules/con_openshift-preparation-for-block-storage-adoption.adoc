[id="openshift-preparation-for-block-storage-adoption_{context}"]

= OpenShift preparation for {block_storage} adoption

As explained in xref:planning-the-new-deployment_{context}[Planning the new deployment], before deploying OpenStack in OpenShift, you must ensure that the networks are ready, that you have decided the node selection, and also make sure any necessary changes to the OpenShift nodes have been made. For Cinder volume and backup services all these 3 must be carefully considered.

Node Selection::
You might need, or want, to restrict the OpenShift nodes where cinder volume and
backup services can run.
+
The best example of when you need to do node selection for a specific cinder
service is when you deploy Cinder with the LVM driver. In that scenario, the
LVM data where the volumes are stored only exists in a specific host, so you
need to pin the cinder-volume service to that specific OpenShift node. Running
the service on any other OpenShift node would not work.  Since `nodeSelector`
only works on labels, you cannot use the OpenShift host node name to restrict
the LVM backend and you need to identify it using a unique label, an existing label, or new label:
+
----
$ oc label nodes worker0 lvm=cinder-volumes
----
+
[source,yaml]
----
apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  secret: osp-secret
  storageClass: local-storage
  cinder:
    enabled: true
    template:
      cinderVolumes:
        lvm-iscsi:
          nodeSelector:
            lvm: cinder-volumes
< . . . >
----
+
As mentioned in the xref:node-selector_{context}[About node selector], an example where you need to use labels is when using FC storage and you do not have HBA cards in all your OpenShift nodes. In this scenario you need to restrict all the cinder volume backends (not only the FC one) as well as the backup services.
+
Depending on the cinder backends, their configuration, and the usage of Cinder,
you can have network intensive cinder volume services with lots of I/O as well as
cinder backup services that are not only network intensive but also memory and
CPU intensive. This may be a concern for the OpenShift human operators, and
they may want to use the `nodeSelector` to prevent these service from
interfering with their other OpenShift workloads. For more information about node selection, see xref:node-selector_{context}[About node selector].
+
When selecting the nodes where cinder volume is going to run please remember
that cinder-volume may also use local storage when downloading a glance image
for the create volume from image operation, and it can require a considerable
amount of space when having concurrent operations and not using cinder volume
cache.
+
If you do not have nodes with enough local disk space for the temporary images, you can use a remote NFS location for the images. You had to manually set this up in Director deployments, but with operators, you can do it
automatically using the extra volumes feature ()`extraMounts`.

Transport protocols::
Due to the specifics of the storage transport protocols some changes may be
required on the OpenShift side, and although this is something that must be
documented by the Vendor here wer are going to provide some generic
instructions that can serve as a guide for the different transport protocols.
+
Check the backend sections in your `cinder.conf` file that are listed in the
`enabled_backends` configuration option to figure out the transport storage
protocol used by the backend.
+
Depending on the backend, you can find the transport protocol:
+
* Looking at the `volume_driver` configuration option, as it may contain the
protocol itself: RBD, iSCSI, FC...
* Looking at the `target_protocol` configuration option
+
WARNING: Any time a `MachineConfig` is used to make changes to OpenShift
nodes the node will reboot!!  Act accordingly.

NFS::
There is nothing to do for NFS. OpenShift can connect to NFS backends without
any additional changes.

RBD/Ceph::
There is nothing to do for RBD/Ceph in terms of preparing the nodes, OpenShift
can connect to Ceph backends without any additional changes. Credentials and
configuration files will need to be provided to the services though.

iSCSI::
Connecting to iSCSI volumes requires that the iSCSI initiator is running on the
OpenShift hosts where volume and backup services are going to run, because
the Linux Open iSCSI initiator does not currently support network namespaces, so
you must only run 1 instance of the service for the normal OpenShift usage, plus
the OpenShift CSI plugins, plus the OpenStack services.
+
If you are not already running `iscsid` on the OpenShift nodes, then you need
to apply a `MachineConfig` similar to this one:
+
[source,yaml]
----
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
    service: cinder
  name: 99-master-cinder-enable-iscsid
spec:
  config:
    ignition:
      version: 3.2.0
    systemd:
      units:
      - enabled: true
        name: iscsid.service
----
+
If you are using labels to restrict the nodes where cinder services are running you need to use a `MachineConfigPool` as described in
the xref:node-selector_{context}[About node selector] to limit the effects of the
`MachineConfig` to only the nodes where your services may run.
+
If you are using a toy single node deployment to test the process, you might need to replace `worker` with `master` in the `MachineConfig`.

//For production deployments using iSCSI volumes, we always recommend setting up
//multipathing, please look at the <<multipathing,multipathing section>> to see
//how to configure it. kgilliga: Commented out because multipathing module doesn't exist yet. Update with xref for beta.

//*TODO:* Add, or at least mention, the Nova eDPM side for iSCSI.

FC::
There is nothing to do for FC volumes to work, but the _cinder volume and cinder
backup services need to run in an OpenShift host that has HBAs_, so if there
are nodes that do not have HBAs then you need to use labels to restrict where
these services can run, as mentioned in the [node selection section]
(#node-selection).
+
This also means that for virtualized OpenShift clusters using FC you need to
expose the host's HBAs inside the VM.

//For production deployments using FC volumes we always recommend setting up
//multipathing, please look at the <<multipathing,multipathing section>> to see
//how to configure it. kgilliga: Commented out because multipathing module doesn't exist yet. Update with xref for beta.

NVMe-oF::
Connecting to NVMe-oF volumes requires that the nvme kernel modules are loaded
on the OpenShift hosts.
+
If you are not already loading the `nvme-fabrics` module on the OpenShift nodes
where volume and backup services are going to run then you need to apply a
`MachineConfig` similar to this one:
+
----
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
    service: cinder
  name: 99-master-cinder-load-nvme-fabrics
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
        - path: /etc/modules-load.d/nvme_fabrics.conf
          overwrite: false
          # Mode must be decimal, this is 0644
          mode: 420
          user:
            name: root
          group:
            name: root
          contents:
            # Source can be a http, https, tftp, s3, gs, or data as defined in rfc2397.
            # This is the rfc2397 text/plain string format
            source: data:,nvme-fabrics
----
+
If you are using labels to restrict the nodes where cinder
services are running, you need to use a `MachineConfigPool` as described in
the xref:node-selector_{context}[About node selector] to limit the effects of the
`MachineConfig` to only the nodes where your services may run.
+
If you are using a toy single node deployment to test the process you migt need to replace `worker` with `master` in the `MachineConfig`.
+
You are only loading the `nvme-fabrics` module because it takes care of loading
the transport specific modules (tcp, rdma, fc) as needed.
+
For production deployments using NVMe-oF volumes it is recommended that you use
multipathing. For NVMe-oF volumes OpenStack uses native multipathing, called
https://nvmexpress.org/faq-items/what-is-ana-nvme-multipathing/[ANA].
+
Once the OpenShift nodes have rebooted and are loading the `nvme-fabrics` module
you can confirm that the Operating System is configured and supports ANA by
checking on the host:
+
----
cat /sys/module/nvme_core/parameters/multipath
----
+
IMPORTANT: ANA doesn't use the Linux Multipathing Device Mapper, but the
*current OpenStack
code requires `multipathd` on compute nodes to be running for Nova to be able to
use multipathing, so please remember to follow the multipathing part for compute
nodes on the <<multipathing,multipathing section>>.

//*TODO:* Add, or at least mention, the Nova eDPM side for NVMe-oF.

Multipathing::
For iSCSI and FC protocols, using multipathing is recommended, which
has 4 parts:

* Prepare the OpenShift hosts
* Configure the Cinder services
* Prepare the Nova computes
* Configure the Nova service
+
To prepare the OpenShift hosts, you need to ensure that the Linux Multipath
Device Mapper is configured and running on the OpenShift hosts, and you do
that using `MachineConfig` like this one:
+
[source,yaml]
----
# Includes the /etc/multipathd.conf contents and the systemd unit changes
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
    service: cinder
  name: 99-master-cinder-enable-multipathd
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
        - path: /etc/multipath.conf
          overwrite: false
          # Mode must be decimal, this is 0600
          mode: 384
          user:
            name: root
          group:
            name: root
          contents:
            # Source can be a http, https, tftp, s3, gs, or data as defined in rfc2397.
            # This is the rfc2397 text/plain string format
            source: data:,defaults%20%7B%0A%20%20user_friendly_names%20no%0A%20%20recheck_wwid%20yes%0A%20%20skip_kpartx%20yes%0A%20%20find_multipaths%20yes%0A%7D%0A%0Ablacklist%20%7B%0A%7D
    systemd:
      units:
      - enabled: true
        name: multipathd.service
----
+
If you are using labels to restrict the nodes where cinder
services are running you need to use a `MachineConfigPool` as described in
the xref:node-selector_{context}[About node selector] to limit the effects of the
`MachineConfig` to only the nodes where your services may run.
+
If you are using a toy single node deployment to test the process you might need to replace `worker` with `master` in the `MachineConfig`.
+
To configure the cinder services to use multipathing you need to enable the
`use_multipath_for_image_xfer` configuration option in all the backend sections
and in the `[DEFAULT]` section for the backup service, but in Podified
deployments you do not need to worry about it, because that's the default. So as
long as you do not override it setting `use_multipath_for_image_xfer = false` then multipathing will work as long as the service is running on the OpenShift host.

//*TODO:* Add, or at least mention, the Nova eDPM side for Multipathing once
//it's implemented.

