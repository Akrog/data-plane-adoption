[id="changes-to-cephFS-via-NFS_{context}"]

= Changes to CephFS via NFS

If the {rhos_prev_long} ({OpenStackShort}) {rhos_prev_ver} deployment uses CephFS via NFS as a backend for {rhos_component_storage_file_first_ref}, there's a `ceph-nfs` service on the {OpenStackShort} controller nodes deployed and managed by {OpenStackPreviousInstaller}. This service cannot be directly imported into {rhos_long} {rhos_curr_ver}. On {rhos_acro} {rhos_curr_ver}, the {rhos_component_storage_file} only supports using a "clustered" NFS service that is directly managed on the Ceph cluster. So, adoption with this service will involve a data path disruption to existing NFS clients. The timing of this disruption can be controlled by the deployer independent of this adoption procedure.

On {OpenStackShort} {rhos_prev_ver}, pacemaker controls the high availability of the `ceph-nfs` service. This service is assigned a Virtual IP (VIP) address that is also managed by pacemaker. The VIP is typically created on an isolated `StorageNFS` network. There are ordering and collocation constraints established between this VIP, `ceph-nfs` and the Shared File Systems service's share manager service on the
controller nodes. Prior to adopting {rhos_component_storage_file}, pacemaker's ordering and collocation constraints must be adjusted to separate the share manager service. This establishes `ceph-nfs` with its VIP as an isolated, standalone NFS service that can be decommissioned at will after completing the {OpenStackShort} adoption.

Red Hat Ceph Storage 7.0 introduced a native `clustered Ceph NFS service`. This service has to be deployed on the Ceph cluster using the Ceph orchestrator prior to adopting the {rhos_component_storage_file}. This NFS service will eventually replace the standalone NFS service from {OpenStackShort} {rhos_prev_ver} in your deployment. When the {rhos_component_storage_file} is adopted into the {rhos_acro} {rhos_curr_ver} environment, it will establish all the existing
exports and client restrictions on the new clustered Ceph NFS service. Clients can continue to read and write data on their existing NFS shares, and are not affected until the old standalone NFS service is decommissioned. This switchover window allows clients to re-mount the same share from the new
clustered Ceph NFS service during a scheduled downtime.

In order to ensure that existing clients can easily switchover to the new NFS
service, it is necessary that the clustered Ceph NFS service is assigned an
IP address from the same isolated `StorageNFS` network. Doing this will ensure that NFS users aren't expected to make any networking changes to their
existing workloads. These users only need to discover and re-mount their shares using new export paths. When the adoption procedure is complete, {OpenStackShort} users can query the {rhos_component_storage_file} API to list the export locations on existing shares to identify the `preferred` paths to mount these shares. These `preferred` paths
will correspond to the new clustered Ceph NFS service in contrast to other
non-preferred export paths that continue to be displayed until the old
isolated, standalone NFS service is decommissioned.

See xref:creating-a-ceph-nfs-cluster_migrating-databases[Creating a Ceph NFS cluster] for instructions on setting up a clustered NFS service.