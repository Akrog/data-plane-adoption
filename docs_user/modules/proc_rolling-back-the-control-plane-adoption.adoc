[id="rolling-back-control-plane-adoption_{context}"]

= Rolling back the control plane adoption

If you encountered a problem during the adoption of the {rhos_prev_long} ({OpenStackShort}) control plane services that prevents you from completing the adoption procedure, you can roll back the control plane adoption.

[IMPORTANT]
The roll back operation is only possible during the
control plane parts of the adoption procedure. If you altered the data plane
nodes in any way during the procedure, the roll back is not possible.

During the control plane adoption, services on the source cloud's
control plane are stopped but not removed. The databases on the source
control plane are not edited by the adoption procedure. The
destination control plane received a copy of the original
control plane databases. The roll back procedure assumes that the data
plane has not yet been touched by the adoption procedure and it is
still connected to the source control plane.

The rollback procedure consists of the following steps:

* Restoring the functionality of the source control plane.

* Removing the partially or fully deployed destination
  control plane.

.Procedure

. To restore the source cloud to a working state, start the {OpenStackShort}
control plane services that you previously stopped during the adoption
procedure:
+
----
ServicesToStart=("tripleo_horizon.service"
                 "tripleo_keystone.service"
                 "tripleo_barbican_api.service"
                 "tripleo_barbican_worker.service"
                 "tripleo_barbican_keystone_listener.service"
                 "tripleo_cinder_api.service"
                 "tripleo_cinder_api_cron.service"
                 "tripleo_cinder_scheduler.service"
                 "tripleo_cinder_volume.service"
                 "tripleo_cinder_backup.service"
                 "tripleo_glance_api.service"
                 "tripleo_manila_api.service"
                 "tripleo_manila_api_cron.service"
                 "tripleo_manila_scheduler.service"
                 "tripleo_neutron_api.service"
                 "tripleo_placement_api.service"
                 "tripleo_nova_api_cron.service"
                 "tripleo_nova_api.service"
                 "tripleo_nova_conductor.service"
                 "tripleo_nova_metadata.service"
                 "tripleo_nova_scheduler.service"
                 "tripleo_nova_vnc_proxy.service"
                 "tripleo_aodh_api.service"
                 "tripleo_aodh_api_cron.service"
                 "tripleo_aodh_evaluator.service"
                 "tripleo_aodh_listener.service"
                 "tripleo_aodh_notifier.service"
                 "tripleo_ceilometer_agent_central.service"
                 "tripleo_ceilometer_agent_compute.service"
                 "tripleo_ceilometer_agent_ipmi.service"
                 "tripleo_ceilometer_agent_notification.service"
                 "tripleo_ovn_cluster_north_db_server.service"
                 "tripleo_ovn_cluster_south_db_server.service"
                 "tripleo_ovn_cluster_northd.service")

PacemakerResourcesToStart=("galera-bundle"
                           "haproxy-bundle"
                           "rabbitmq-bundle"
                           "openstack-cinder-volume"
                           "openstack-cinder-backup"
                           "openstack-manila-share")

echo "Starting systemd OpenStack services"
for service in ${ServicesToStart[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            if ${!SSH_CMD} sudo systemctl is-enabled $service &> /dev/null; then
                echo "Starting the $service in controller $i"
                ${!SSH_CMD} sudo systemctl start $service
            fi
        fi
    done
done

echo "Checking systemd OpenStack services"
for service in ${ServicesToStart[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            if ${!SSH_CMD} sudo systemctl is-enabled $service &> /dev/null; then
                if ! ${!SSH_CMD} systemctl show $service | grep ActiveState=active >/dev/null; then
                    echo "ERROR: Service $service is not running on controller $i"
                else
                    echo "OK: Service $service is running in controller $i"
                fi
            fi
        fi
    done
done

echo "Starting pacemaker OpenStack services"
for i in {1..3}; do
    SSH_CMD=CONTROLLER${i}_SSH
    if [ ! -z "${!SSH_CMD}" ]; then
        echo "Using controller $i to run pacemaker commands"
        for resource in ${PacemakerResourcesToStart[*]}; do
            if ${!SSH_CMD} sudo pcs resource config $resource &>/dev/null; then
                echo "Starting $resource"
                ${!SSH_CMD} sudo pcs resource enable $resource
            else
                echo "Service $resource not present"
            fi
        done
        break
    fi
done

echo "Checking pacemaker OpenStack services"
for i in {1..3}; do
    SSH_CMD=CONTROLLER${i}_SSH
    if [ ! -z "${!SSH_CMD}" ]; then
        echo "Using controller $i to run pacemaker commands"
        for resource in ${PacemakerResourcesToStop[*]}; do
            if ${!SSH_CMD} sudo pcs resource config $resource &>/dev/null; then
                if ${!SSH_CMD} sudo pcs resource status $resource | grep Started >/dev/null; then
                    echo "OK: Service $resource is started"
                else
                    echo "ERROR: Service $resource is stopped"
                fi
            fi
        done
        break
    fi
done
----

. If the Ceph NFS service is running on the deployment as a {rhos_component_storage_file_first_ref}
backend, you must restore the pacemaker ordering and colocation constraints
involving the "openstack-manila-share" service:
+
----

sudo pcs constraint order start ceph-nfs then openstack-manila-share kind=Optional id=order-ceph-nfs-openstack-manila-share-Optional
sudo pcs constraint colocation add openstack-manila-share with ceph-nfs score=INFINITY id=colocation-openstack-manila-share-ceph-nfs-INFINITY

----

. Verify that the source cloud is operational again, e.g. by
running `openstack` CLI commands or using the {dashboard_first_ref}.
//kgilliga: Should we be more specific? Which CLI commands?

. Remove the partially or fully deployed control plane so that another adoption attempt can be made later:
+
----
oc delete --ignore-not-found=true --wait=false openstackcontrolplane/openstack
oc patch openstackcontrolplane openstack --type=merge --patch '
metadata:
  finalizers: []
' || true

while oc get pod | grep rabbitmq-server-0; do
    sleep 2
done
while oc get pod | grep openstack-galera-0; do
    sleep 2
done

oc delete --ignore-not-found=true --wait=false pod mariadb-copy-data
oc delete --ignore-not-found=true --wait=false pvc mariadb-data
oc delete --ignore-not-found=true --wait=false pod ovn-copy-data
oc delete --ignore-not-found=true secret osp-secret
----

[NOTE]
Since restoring the source control plane services, their internal
state may have changed. Before retrying the adoption procedure, it is
important to verify that the control plane resources have
been removed and there are no leftovers which could affect the
following adoption procedure attempt. Notably, the previously created
copies of the database contents must not be used in another adoption
attempt, and new copies must be made according to the adoption
procedure documentation.

//kgilliga: How do users verify that the control plane resources have all been removed?
